import pandas as pd
import time
import math
import random
from typing import List, Tuple, Dict, Callable
import copy
def read_dataset(filepath='housing.csv'):
  try:
    data=pd.read_csv(filepath)
    return data
  except Exception as e:
        print(f"Error loading dataset: {e}")
        return None

  def preprocess_data(data, test_size=0.2, random_state=42):
    df = data.copy()
    
    for column in df.columns:
        if df[column].isnull().sum() > 0:
            df[column].fillna(df[column].median(), inplace=True)
    X = df.drop('median_house_value', axis=1) if 'median_house_value' in df.columns else df.iloc[:, :-1]
    y = df['median_house_value'] if 'median_house_value' in df.columns else df.iloc[:, -1]
    
    # Handle categorical variables (if any)
    categorical_cols = [col for col in X.columns if X[col].dtype == 'object']
    for col in categorical_cols:

        dummies = pd.get_dummies(X[col], prefix=col, drop_first=True)
        X = pd.concat([X.drop(col, axis=1), dummies], axis=1)
    

    X_normalized, normalization_params = normalize_features(X)
    

    X_normalized.insert(0, 'bias', 1)
    

    X_array = X_normalized.values.tolist()
    y_array = y.values.tolist()
    
    X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, test_size=test_size, random_state=random_state)
    
    return {
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train,
        'y_test': y_test,
        'feature_names': X_normalized.columns.tolist(),
        'normalization_params': normalization_params
    }
def normalize_features(X):
    normalization_params = {}
    X_normalized = X.copy()
    
    for column in X.columns:
        # Skip bias column if it exists
        if column == 'bias':
            continue
            
        mean = X[column].mean()
        std = X[column].std()
        
        # Handle zero std (constant features)
        if std == 0:
            std = 1
            
        X_normalized[column] = (X[column] - mean) / std
        normalization_params[column] = {'mean': mean, 'std': std}
    
    return X_normalized, normalization_params
def train_test_split(X, y, test_size=0.2, random_state=42):

    # Set random seed for reproducibility
    random.seed(random_state)
    
    # Create indices and shuffle
    indices = list(range(len(X)))
    random.shuffle(indices)
    
    # Calculate split point
    split_point = int(len(X) * (1 - test_size))
    
    # Split data
    train_indices = indices[:split_point]
    test_indices = indices[split_point:]
    
    X_train = [X[i] for i in train_indices]
    X_test = [X[i] for i in test_indices]
    y_train = [y[i] for i in train_indices]
    y_test = [y[i] for i in test_indices]
    
    return X_train, X_test, y_train, y_test
def dot_product(v1, v2):
    return sum(x * y for x, y in zip(v1, v2))

def matrix_vector_multiply(matrix, vector):
    """Multiply a matrix by a vector"""
    return [dot_product(row, vector) for row in matrix]

def transpose(matrix):
    """Transpose a matrix"""
    # Get dimensions
    if not matrix:
        return []
    rows = len(matrix)
    cols = len(matrix[0])
    
    # Create transposed matrix
    result = [[0 for _ in range(rows)] for _ in range(cols)]
    
    # Fill transposed matrix
    for i in range(rows):
        for j in range(cols):
            result[j][i] = matrix[i][j]
    
    return result
def vector_subtraction(v1, v2):
    """Subtract vector v2 from v1"""
    return [a - b for a, b in zip(v1, v2)]
def vector_addition(v1, v2):
    """Add two vectors"""
    return [a + b for a, b in zip(v1, v2)]

def scalar_multiply(scalar, vector):
    """Multiply a vector by a scalar"""
    return [scalar * x for x in vector]

def mean_squared_error(y_true, y_pred):
    """Calculate MSE between true and predicted values"""
    return sum((true - pred) ** 2 for true, pred in zip(y_true, y_pred)) / len(y_true)

def r_squared(y_true, y_pred):
    """Calculate R^2 score"""
    mean_y = sum(y_true) / len(y_true)
    ss_total = sum((y - mean_y) ** 2 for y in y_true)
    ss_residual = sum((y - y_hat) ** 2 for y, y_hat in zip(y_true, y_pred))
    
    # Handle edge case where ss_total is 0
    if ss_total == 0:
        return 0
        
    return 1 - (ss_residual / ss_total)

def predict(X, weights):
    """Make predictions using the linear model"""
    return [dot_product(x, weights) for x in X]
def batch_gradient_descent(X, y, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):
    # Initialize weights to zeros
    n_features = len(X[0])
    weights = [0.0] * n_features
    
    # Training history
    history = {
        'iterations': [],
        'mse': [],
        'weights': []
    }
    
    # Start timer
    start_time = time.time()
    
    # Gradient descent loop
    for iteration in range(max_iterations):
        # Make predictions
        y_pred = predict(X, weights)
        
        # Calculate error
        errors = vector_subtraction(y_pred, y)
        
        # Calculate MSE
        mse = mean_squared_error(y, y_pred)
        
        # Record history
        history['iterations'].append(iteration)
        history['mse'].append(mse)
        history['weights'].append(copy.deepcopy(weights))
        
        # Check for convergence
        if iteration > 0 and abs(history['mse'][iteration-1] - mse) < tolerance:
            print(f"Converged after {iteration} iterations")
            break
            
        # Calculate gradients (partial derivatives)
        gradients = [0.0] * n_features
        for j in range(n_features):
            for i in range(len(X)):
                gradients[j] += errors[i] * X[i][j]
            gradients[j] /= len(X)
        
        # Update weights using gradient descent
        for j in range(n_features):
            weights[j] -= learning_rate * gradients[j]
    
    # End timer
    end_time = time.time()
    training_time = end_time - start_time
    
    print(f"Batch Gradient Descent completed in {training_time:.4f} seconds")
    print(f"Final MSE: {history['mse'][-1]:.6f}")
    
    return weights, history, training_time
def evaluate_model(X_test, y_test, weights):

    # Make predictions
    y_pred = predict(X_test, weights)
    
    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    r2 = r_squared(y_test, y_pred)
    
    # Calculate RMSE (more interpretable)
    rmse = math.sqrt(mse)
    
    # Calculate MAE
    mae = sum(abs(true - pred) for true, pred in zip(y_test, y_pred)) / len(y_test)
    
    return {
        'mse': mse,
        'rmse': rmse,
        'mae': mae,
        'r2': r2
    }

def compare_models(data_dict, learning_rates=[0.01, 0.001, 0.0001], 
                  batch_sizes=[32, 64, 128]):
    """
    Compare different linear regression implementations with various hyperparameters
    """
    X_train = data_dict['X_train']
    y_train = data_dict['y_train']
    X_test = data_dict['X_test']
    y_test = data_dict['y_test']
    
    results = []
    
    # Test Batch Gradient Descent with different learning rates
    for lr in learning_rates:
        print(f"\nRunning Batch Gradient Descent with learning_rate={lr}")
        weights, history, train_time = batch_gradient_descent(
            X_train, y_train, learning_rate=lr, max_iterations=1000)
        metrics = evaluate_model(X_test, y_test, weights)
        
        results.append({
            'algorithm': 'Batch GD',
            'learning_rate': lr,
            'batch_size': 'N/A',
            'training_time': train_time,
            'iterations': len(history['iterations']),
            'final_mse_train': history['mse'][-1],
            'test_metrics': metrics
        })
    return results
def main():
    data=read_dataset()

    data_dict = preprocess_data(data)
    
    # Compare models
    print("\nComparing linear regression implementations...")
    results = compare_models(data_dict)
    
    print("\nAnalysis complete!")
    return results

def create_synthetic_housing_data(n_samples=1000):
    """Create synthetic housing data for demonstration purposes"""
    print("Creating synthetic California Housing dataset...")
    
    # Set random seed for reproducibility
    random.seed(42)
    
    # Create feature names similar to California Housing
    columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',
              'total_bedrooms', 'population', 'households', 'median_income',
              'median_house_value']
    
    # Generate synthetic data
    data = []
    for _ in range(n_samples):
        # Generate correlated features
        longitude = random.uniform(-124.3, -114.3)
        latitude = random.uniform(32.5, 42.5)
        housing_age = random.uniform(1, 52)
        rooms = random.uniform(2, 39320)
        bedrooms = rooms * random.uniform(0.1, 0.4)
        population = random.uniform(3, 35682)
        households = random.uniform(1, 6082)
        income = random.uniform(0.5, 15)
        
        # Generate target with some noise
        base_price = 150000 + income * 70000 - longitude * 20000 + latitude * 10000 - housing_age * 2000
        house_value = base_price + random.uniform(-50000, 50000)
        
        # Append to data
        data.append([longitude, latitude, housing_age, rooms, bedrooms, 
                    population, households, income, house_value])
    
    # Create pandas DataFrame
    df = pd.DataFrame(data, columns=columns)
    print(f"Created synthetic dataset with {df.shape[0]} rows and {df.shape[1]} columns")
    
    return df

if __name__ == "__main__":
    results = main()
